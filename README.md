# WEB_CRAWLER
A web crawler written in Rust
## Description
A high-performance web crawler built in Rust, designed for efficiently scraping data from websites. This tool is optimized for speed, parallel processing, and minimal memory usage, leveraging Rustâ€™s safety and concurrency features.

Features
* Multi-threaded: Leverages Rust's async capabilities for crawling multiple pages concurrently.
* Customizable: Specify target websites, scraping rules, and depth of crawling.
* Error Handling: Graceful recovery from connection timeouts, HTTP errors, and missing resources.
* Efficient Parsing: Uses minimal memory while parsing large websites.
* Logging: Provides detailed logs on crawl status, errors, and extracted data.
* Extensible: Easily add features like data extraction, filtering, or integrations with other systems.

## Install

```
git clone https://github.com/Cru-zen/WEB_CRAWLER.git
```
